{"version":3,"file":"static/js/8781.ba5ad761.chunk.js","mappings":"oMAGM,SAAUA,EAAaC,GAC3B,GACEA,EAAKC,YAAYC,OAAOC,mBACxBH,EAAKI,SAASF,OAAOG,kBAErB,MAAM,IAAIC,MAAM,oBAElB,OAAON,EAAKO,UACd,CAEA,MAAMC,UAAmBF,OAcnB,SAAUG,EAAiBC,GAC/B,GAAKA,GAIDA,EAAOC,QAAS,CAClB,GAA4B,qBAAjBC,aAET,MAAM,IAAIA,aAAa,UAAW,cAC7B,CACL,MAAMC,EAAI,IAAIL,EAAW,WAEzB,MADAK,EAAEC,KAAO,cACHD,CACR,CACF,CACF,CAoBM,SAAUE,EAAeC,EAAiBC,GAC9C,MAAMC,EAAwB,GAC9B,IAAIC,EAA0B,KAE9B,OAAsB,IAAlBH,EAAOI,OACFJ,GAGTA,EAAOK,MAAK,SAAUC,EAAIC,GACxB,MAAMC,EAAMF,EAAGG,KAAKC,cAAgBH,EAAGE,KAAKC,cAC5C,OAAe,IAARF,EAAYA,EAAMF,EAAGG,KAAKE,aAAeJ,EAAGE,KAAKE,YAC1D,IAEAX,EAAOY,SAAQC,IApBX,IAAyBC,EAAeC,IAqBrCd,GAAUY,EAAMG,KAAKC,UAAUhB,GAAU,KAC1B,OAAdE,GACFD,EAAagB,KAAKL,GAClBV,EAAYU,IAxBWC,EA0BJX,GA1BmBY,EA0BRF,GAxB3BJ,KAAKC,cAAgBI,EAAOE,KAAKN,cAAgB,MACxDK,EAAOC,KAAKN,cAAgBI,EAAOL,KAAKC,cAAgB,IAwB9CG,EAAMG,KAAKC,UAAUd,EAAUa,MAAQ,IACzCb,EAAUa,KAAOH,EAAMG,OAGzBd,EAAagB,KAAKL,GAClBV,EAAYU,IAGlB,IAGKX,EACT,C,wBC7Fc,MAAOiB,EAGnBC,WAAAA,CAAYV,EAAuBC,GACjCU,KAAKX,cAAgBA,EACrBW,KAAKV,aAAeA,CACtB,CAEAW,QAAAA,GACE,MAAO,GAAPC,OAAUF,KAAKX,cAAa,KAAAa,OAAIF,KAAKV,aACvC,CAEAM,SAAAA,CAAUO,GACR,OACEH,KAAKX,cAAgBc,EAAEd,eAAiBW,KAAKV,aAAea,EAAEb,YAElE,CAEA,UAAOc,GACL,IAAIA,EACAC,EAAI,EAAC,QAAAC,EAAAC,UAAAxB,OAFGyB,EAAqB,IAAAC,MAAAH,GAAAI,EAAA,EAAAA,EAAAJ,EAAAI,IAArBF,EAAqBE,GAAAH,UAAAG,GAGjC,MAAQN,EAAKC,GAAK,EAChBD,EAAMI,EAAKH,GAEb,KAAOA,EAAIG,EAAKzB,OAAQsB,GAAK,EACvBD,EAAIR,UAAUY,EAAKH,IAAM,IAC3BD,EAAMI,EAAKH,IAGf,OAAOD,CACT,EAEI,SAAUO,EAAUC,GAA4C,IAA7BC,EAAMN,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAG,EAChD,GAD4DA,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,IAAAA,UAAA,GAE1D,MAAM,IAAItC,MAAM,mDAGlB,OAAO,IAAI6B,EACW,cAApBc,EAAMC,EAAS,GACO,WAApBD,EAAMC,EAAS,GACK,SAApBD,EAAMC,EAAS,GACK,MAApBD,EAAMC,EAAS,GACK,IAApBD,EAAMC,EAAS,GACfD,EAAMC,EAAS,GAChBD,EAAMC,EAAS,IAAM,EAAKD,EAAMC,GAErC,CC5Cc,MAAOE,EAMnBhB,WAAAA,CACEX,EACAO,EACAqB,GACuB,IAAvBC,EAAWV,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,QAAGO,EAEdd,KAAKZ,KAAOA,EACZY,KAAKL,KAAOA,EACZK,KAAKgB,IAAMA,EACXhB,KAAKkB,aAAeD,CACtB,CAEAE,cAAAA,GAEE,MAAO,GAAPjB,OAAUF,KAAKZ,KAAI,MAAAc,OAAKF,KAAKL,KAAI,UAAAO,OAC/BF,KAAKgB,IACP,kBAAAd,OAAiBF,KAAKiB,cAAa,IACrC,CAEAhB,QAAAA,GACE,OAAOD,KAAKmB,gBACd,CAEAvB,SAAAA,CAAUO,GACR,OACEH,KAAKZ,KAAKQ,UAAUO,EAAEf,OACtBY,KAAKL,KAAKC,UAAUO,EAAER,OACtBK,KAAKgB,IAAMb,EAAEa,GAEjB,CAEAC,WAAAA,GACE,YAA0BH,IAAtBd,KAAKkB,aACAlB,KAAKkB,aAEPlB,KAAKL,KAAKN,cAAgB,MAAYW,KAAKZ,KAAKC,aACzD,ECzBY,MAAgB+B,EAK5BrB,WAAAA,CAAAsB,GAMC,IANW,WACVC,EAAU,cACVC,EAAiBC,IAAcA,IAIhCH,EACCrB,KAAKsB,WAAaA,EAClBtB,KAAKyB,aAAeF,CACtB,CAMO,iBAAMG,GAA8B,IAAlBC,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EACxC,MAAQqB,QAASC,KAAaC,SAAe9B,KAAK+B,MAAMJ,GACxD,OAAOG,CACT,CASAE,cAAAA,CACEC,EACAC,GAEA,OAAID,EACKA,EAAWrC,UAAUsC,GAAiB,EACzCA,EACAD,EAEGC,CAEX,CAEA,WAAMH,GAAwB,IAAlBJ,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAO3B,OANKP,KAAKmC,SACRnC,KAAKmC,OAASnC,KAAKoC,OAAOT,GAAMU,OAAM7D,IAEpC,MADAwB,KAAKmC,YAASrB,EACRtC,CAAC,KAGJwB,KAAKmC,MACd,CAEA,eAAMG,CAAUC,GAAiC,IAAlBZ,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,E,MAE9C,SAA2B,QAAlBiC,SADSxC,KAAK+B,MAAMJ,IAChBC,QAAQW,UAAM,IAAAC,OAAA,EAAAA,EAAEC,SAC/B,ECnDY,MAAOC,UAAmBtB,EACtC,eAAMuB,CAAUC,GAAmC,IAAlBjB,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAChD,MAAMsC,QAAkB7C,KAAK+B,MAAMJ,GACnC,IAAKkB,EACH,OAAQ,EAEV,MAAMC,EAAQD,EAAUE,YAAYH,GAEpC,IADYC,EAAUjB,QAAQkB,GAE5B,OAAQ,EAEV,MAAM,MAAEE,GAAUH,EAAUjB,QAAQkB,GACpC,OAAIE,EACKA,EAAML,WAEP,CACV,CAGA,YAAMP,GAAyB,IAAlBT,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAC5B,MAAM0C,QAAYjD,KAAKsB,WAAW4B,SAASvB,GACrCf,QAAcuC,EAAAA,EAAAA,OAAMF,GAI1B,GAHA7E,EAAiBuD,EAAKtD,QAzCR,WA4CVuC,EAAMwC,aAAa,GACrB,MAAM,IAAInF,MAAM,kBAKlB,MAAMoF,EAAWzC,EAAM0C,YAAY,GAC7BC,EAAc3C,EAAM0C,YAAY,GAChCE,EACU,MAAdD,EAAwB,uBAAyB,iBAM7CE,EALqC,CACzC,EAAG,UACH,EAAG,MACH,EAAG,OAEmC,GAAdF,GAC1B,IAAKE,EACH,MAAM,IAAIxF,MAAM,qCAADiC,OAAsCqD,IAEvD,MAAMG,EAAgB,CACpBC,IAAK/C,EAAM0C,YAAY,IACvBM,MAAOhD,EAAM0C,YAAY,IACzBO,IAAKjD,EAAM0C,YAAY,KAEnBQ,EAAYlD,EAAM0C,YAAY,IAE9BS,EAAe,MAEfC,EAAWF,EAAYG,OAAOC,aAAaJ,GAAa,KACxDK,EAAYvD,EAAM0C,YAAY,IAG9Bc,EAAoBxD,EAAM0C,YAAY,KACtC,YAAEP,EAAW,YAAEsB,GAAgBrE,KAAKsE,gBACxC1D,EAAM2D,MAAM,GAAI,GAAKH,IAIvB,IACII,EADAC,EAAa,GAAKL,EAiDtB,MAAO,CACLxC,QAhDc,IAAInB,MAAM4C,GAAUqB,KAAK,GAAGC,KAAI,KAE9C,MAAMC,EAAWhE,EAAM0C,YAAYmB,GACnCA,GAAc,EACd,MAAMhC,EAAoC,CAAC,EAC3C,IAAIO,EACJ,IAAK,IAAI6B,EAAI,EAAGA,EAAID,EAAUC,GAAK,EAAG,CACpC,MAAM7D,EAAMJ,EAAMwC,aAAaqB,GAE/B,GADAA,GAAc,EACVzD,EAAM+C,MACR,MAAM,IAAI9F,MACR,8DAEG,GAAY8F,QAAR/C,EAA0B,CACnC,MAAM8D,EAAalE,EAAM0C,YAAYmB,GACrCA,GAAc,EACK,IAAfK,IACF9B,EAAQhD,KAAK+E,eAAenE,EAAO6D,IAErCA,GAAc,GAAKK,CACrB,KAAO,CACL,MAAMA,EAAalE,EAAM0C,YAAYmB,GACrCA,GAAc,EACd,MAAM9F,EAAS,IAAI8B,MAAMqE,GACzB,IAAK,IAAIE,EAAI,EAAGA,EAAIF,EAAYE,GAAK,EAAG,CACtC,MAAMC,EAAItE,EAAUC,EAAO6D,GACrBS,EAAIvE,EAAUC,EAAO6D,EAAa,GACxCA,GAAc,GACdD,EAAgBxE,KAAKgC,eAAewC,EAAeS,GACnDtG,EAAOqG,GAAK,IAAIjE,EAAMkE,EAAGC,EAAGlE,EAC9B,CACAyB,EAASzB,GAAOrC,CAClB,CACF,CAGA,MAAMwG,EAAcvE,EAAM0C,YAAYmB,GACtCA,GAAc,EACd,MAAMW,EAAc,IAAI3E,MAAM0E,GAC9B,IAAK,IAAIH,EAAI,EAAGA,EAAIG,EAAaH,GAAK,EACpCI,EAAYJ,GAAKrE,EAAUC,EAAO6D,GAClCA,GAAc,EACdD,EAAgBxE,KAAKgC,eAAewC,EAAeY,EAAYJ,IAEjE,MAAO,CAAEvC,WAAU2C,cAAapC,QAAO,IAKvCgB,WACAD,eACAsB,aAhEmB,UAiEnBlB,YACAK,gBACAd,gBACAF,iBACAC,SACAY,cACAtB,cACAuC,aAAc,MAElB,CAEAP,cAAAA,CAAenE,EAAeC,GAO5B,MAAO,CAAE8B,UANSjF,EAChB6H,IAAAA,YACE3E,EAAM2D,MAAM1D,EAAS,GAAIA,EAAS,KAClC,IAIN,CAEAyD,eAAAA,CAAgBkB,GACd,IAAIC,EAAY,EACZC,EAAgB,EACpB,MAAMrB,EAAwB,GACxBtB,EAAsC,CAAC,EAC7C,IAAK,IAAI1C,EAAI,EAAGA,EAAImF,EAAWzG,OAAQsB,GAAK,EAC1C,IAAKmF,EAAWnF,GAAI,CAClB,GAAIqF,EAAgBrF,EAAG,CACrB,IAAIuC,EAAU4C,EAAWvF,SAAS,OAAQyF,EAAerF,GACzDuC,EAAU5C,KAAKyB,aAAamB,GAC5ByB,EAAYoB,GAAa7C,EACzBG,EAAYH,GAAW6C,CACzB,CACAC,EAAgBrF,EAAI,EACpBoF,GAAa,CACf,CAEF,MAAO,CAAE1C,cAAasB,cACxB,CAEA,oBAAMsB,CACJ/C,EACAxC,EACAwF,GACkB,IAAlBjE,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAEbH,EAAM,IACRA,EAAM,GAGR,MAAMyC,QAAkB7C,KAAK+B,MAAMJ,GACnC,IAAKkB,EACH,MAAO,GAET,MAAMC,EAAQD,EAAUE,YAAYH,GAC9BiD,EAAKhD,EAAUjB,QAAQkB,GAC7B,IAAK+C,EACH,MAAO,IAGSA,EAAGT,YAAYrG,OAC7B8G,EAAGT,YACDhF,GAtMa,IAsMYyF,EAAGT,YAAYrG,OACpC8G,EAAGT,YAAYrG,OAAS,EACxBqB,GAxMS,IA0Mf,IAAIN,EAAc,EAAG,KAEvBgG,QAAQC,KAAK,4CAKf,MAAMC,GA5MQC,EA4MmB7F,EA5MNyD,EA4MW+B,EAzMjC,CACL,CAAC,EAAG,GACJ,CAAC,IAJHK,GAAO,IAIQ,IAAK,IAHpBpC,GAAO,IAGyB,KAC9B,CAAC,GAAKoC,GAAO,IAAK,GAAKpC,GAAO,KAC9B,CAAC,IAAMoC,GAAO,IAAK,IAAMpC,GAAO,KAChC,CAAC,KAAOoC,GAAO,IAAK,KAAOpC,GAAO,KAClC,CAAC,MAAQoC,GAAO,IAAK,MAAQpC,GAAO,OATxC,IAAkBoC,EAAapC,EA6M3B,MAAMlF,EAAkB,GAGxB,IAAK,MAAOiF,EAAOC,KAAQmC,EACzB,IAAK,IAAIhF,EAAM4C,EAAO5C,GAAO6C,EAAK7C,IAChC,GAAI6E,EAAGpD,SAASzB,GACd,IAAK,MAAMkF,KAAKL,EAAGpD,SAASzB,GAC1BrC,EAAOkB,KAAK,IAAIkB,EAAMmF,EAAE9G,KAAM8G,EAAEvG,KAAMqB,IAQ9C,MAAMmF,EAAQN,EAAGT,YAAYrG,OAC7B,IAAIH,EAAS,KACb,MAAMwH,EAASC,KAAKjG,IAAIA,GAAO,GAAI+F,EAAQ,GACrCG,EAASD,KAAKjG,IAAIwF,GAAO,GAAIO,EAAQ,GAC3C,IAAK,IAAI9F,EAAI+F,EAAQ/F,GAAKiG,IAAUjG,EAAG,CACrC,MAAMkG,EAAKV,EAAGT,YAAY/E,GACtBkG,KACG3H,GAAU2H,EAAG3G,UAAUhB,GAAU,KACpCA,EAAS2H,EAGf,CAEA,OAAO7H,EAAeC,EAAQC,EAChC,ECxOF,SAAS4H,EAAOC,EAAaC,GAC3B,OAAOL,KAAKM,MAAMF,EAAM,GAAKC,EAC/B,CAEc,MAAOE,UAAYxF,EAI/BrB,WAAAA,CAAYS,GACVqG,MAAMrG,GACNR,KAAK+D,aAAe,EACpB/D,KAAK8G,MAAQ,EACb9G,KAAK+G,SAAW,CAClB,CACA,eAAMpE,CAAUC,GAAmC,IAAlBjB,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAChD,MAAMsC,QAAkB7C,KAAK+B,MAAMJ,GACnC,IAAKkB,EACH,OAAQ,EAEV,MAAMC,EAAQD,EAAUE,YAAYH,GAEpC,IADYC,EAAUjB,QAAQkB,GAE5B,OAAQ,EAEV,MAAM,MAAEE,GAAUH,EAAUjB,QAAQkB,GACpC,OAAIE,EACKA,EAAML,WAEP,CACV,CACAqE,QAAAA,GACE,MAAM,IAAI/I,MAAM,sCAClB,CAEAgJ,YAAAA,CAAarG,EAAeC,GAC1B,MAAM0C,EAAc3C,EAAM0C,YAAYzC,GAChC2C,EACU,MAAdD,EAAwB,uBAAyB,iBAC7CE,EAAS,CAAE,EAAG,UAAW,EAAG,MAAO,EAAG,OAAsB,GAAdF,GACpD,IAAKE,EACH,MAAM,IAAIxF,MAAM,qCAADiC,OAAsCqD,IAEvD,MAAMG,EAAgB,CACpBC,IAAK/C,EAAM0C,YAAYzC,EAAS,GAChC+C,MAAOhD,EAAM0C,YAAYzC,EAAS,GAClCgD,IAAKjD,EAAM0C,YAAYzC,EAAS,KAE5BiD,EAAYlD,EAAM0C,YAAYzC,EAAS,IACvCmD,EAAWF,EAAYG,OAAOC,aAAaJ,GAAa,KACxDK,EAAYvD,EAAM0C,YAAYzC,EAAS,IACvCuD,EAAoBxD,EAAM0C,YAAYzC,EAAS,KAE/C,YAAEwD,EAAW,YAAEtB,GAAgB/C,KAAKsE,gBACxC1D,EAAM2D,MAAM1D,EAAS,GAAIA,EAAS,GAAKuD,IAGzC,MAAO,CACLC,cACAtB,cACAoB,YACAH,WACAN,gBACAD,SACAD,iBAEJ,CAEAc,eAAAA,CAAgBkB,GACd,IAAIC,EAAY,EACZC,EAAgB,EACpB,MAAMrB,EAAc,GACdtB,EAAsC,CAAC,EAC7C,IAAK,IAAI1C,EAAI,EAAGA,EAAImF,EAAWzG,OAAQsB,GAAK,EAC1C,IAAKmF,EAAWnF,GAAI,CAClB,GAAIqF,EAAgBrF,EAAG,CACrB,IAAIuC,EAAU4C,EAAWvF,SAAS,OAAQyF,EAAerF,GACzDuC,EAAU5C,KAAKyB,aAAamB,GAC5ByB,EAAYoB,GAAa7C,EACzBG,EAAYH,GAAW6C,CACzB,CACAC,EAAgBrF,EAAI,EACpBoF,GAAa,CACf,CAEF,MAAO,CAAE1C,cAAasB,cACxB,CAIA,YAAMjC,GAAyB,IAAlBT,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAC5B,MAAMK,QAAcuC,EAAAA,EAAAA,aAAYnD,KAAKsB,WAAW4B,SAASvB,IAGzD,IAAIuF,EACJ,GApGe,WAoGXtG,EAAMwC,aAAa,GACrB8D,EAAa,MACR,IArGQ,WAqGJtG,EAAMwC,aAAa,GAG5B,MAAM,IAAInF,MAAM,kBAFhBiJ,EAAa,CAIf,CAEAlH,KAAK+G,SAAWnG,EAAM0C,YAAY,GAClCtD,KAAK8G,MAAQlG,EAAM0C,YAAY,GAC/BtD,KAAK+D,eAAiB,GAAyB,GAAlB/D,KAAK8G,MAAQ,IAAW,GAAK,EAC1D,MAAMzB,EAAe,IAAMrF,KAAK+G,SAAwB,EAAb/G,KAAK8G,OAC1CK,EAAYvG,EAAM0C,YAAY,IAC9B8D,EACJD,GAAaA,GAAa,GACtBnH,KAAKiH,aAAarG,EAAO,IACzB,CACEyD,YAAa,GACbtB,YAAa,CAAC,EACdiB,SAAU,KACVN,cAAe,CAAEC,IAAK,EAAGC,MAAO,EAAGC,IAAK,GACxCL,eAAgB,uBAChBC,OAAQ,WAEVJ,EAAWzC,EAAM0C,YAAY,GAAK6D,GAGxC,IAAI3C,EACAC,EAAa,GAAK0C,EAAY,EAClC,MAAMvF,EAAU,IAAInB,MAAM4C,GAAUqB,KAAK,GAAGC,KAAI,KAE9C,MAAMC,EAAWhE,EAAM0C,YAAYmB,GACnCA,GAAc,EACd,MAAMhC,EAAoC,CAAC,EAC3C,IAAIO,EACJ,IAAK,IAAI6B,EAAI,EAAGA,EAAID,EAAUC,GAAK,EAAG,CACpC,MAAM7D,EAAMJ,EAAMwC,aAAaqB,GAC/B,GAAIzD,EAAMhB,KAAK+D,aAGbf,EAAQhD,KAAK+E,eAAenE,EAAO6D,EAAa,GAChDA,GAAc,OACT,CACL,MAAM4C,EAAU1G,EAAUC,EAAO6D,EAAa,GAC9CD,EAAgBxE,KAAKgC,eAAewC,EAAe6C,GACnD,MAAMvC,EAAalE,EAAM0C,YAAYmB,EAAa,IAClDA,GAAc,GACd,MAAM9F,EAAS,IAAI8B,MAAMqE,GACzB,IAAK,IAAIE,EAAI,EAAGA,EAAIF,EAAYE,GAAK,EAAG,CACtC,MAAMC,EAAItE,EAAUC,EAAO6D,GACrBS,EAAIvE,EAAUC,EAAO6D,EAAa,GACxCA,GAAc,GAEd9F,EAAOqG,GAAK,IAAIjE,EAAMkE,EAAGC,EAAGlE,EAC9B,CACAyB,EAASzB,GAAOrC,CAClB,CACF,CAEA,MAAO,CAAE8D,WAAUO,QAAO,IAG5B,MAAO,IACFoE,EACHE,KAAK,EACLjE,WACAiC,aAAc,MACdd,gBACA0C,aACAtF,UACAkF,MAAO9G,KAAK8G,MACZ/C,aAAc/D,KAAK+D,aACnBsB,eAEJ,CAEAN,cAAAA,CAAenE,EAAeC,GAO5B,MAAO,CAAE8B,UANSjF,EAChB6H,IAAAA,YACE3E,EAAM2D,MAAM1D,EAAS,GAAIA,EAAS,KAClC,IAIN,CAEA,oBAAM8E,CACJ/C,EACAxC,EACAwF,GACkB,IAAlBjE,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAEbH,EAAM,IACRA,EAAM,GAGR,MAAMyC,QAAkB7C,KAAK+B,MAAMJ,GACnC,IAAKkB,EACH,MAAO,GAET,MAAMC,EAAQD,EAAUE,YAAYH,GAC9BiD,EAAKhD,EAAUjB,QAAQkB,GAC7B,IAAK+C,EACH,MAAO,GAKT,MAAMG,EAAkBhG,KAAKuH,SAASnH,EAAKwF,GACrCjH,EAAkB,GAGxB,IAAK,MAAOiF,EAAOC,KAAQmC,EACzB,IAAK,IAAIhF,EAAM4C,EAAO5C,GAAO6C,EAAK7C,IAChC,GAAI6E,EAAGpD,SAASzB,GACd,IAAK,MAAMkF,KAAKL,EAAGpD,SAASzB,GAC1BrC,EAAOkB,KAAK,IAAIkB,EAAMmF,EAAE9G,KAAM8G,EAAEvG,KAAMqB,IAM9C,OAAOtC,EAAeC,EAAQ,IAAImB,EAAc,EAAG,GACrD,CAKAyH,QAAAA,CAAStB,EAAapC,IACpBoC,GAAO,GACG,IACRA,EAAM,GAEJpC,EAAM,GAAK,KACbA,EAAM,GAAK,IAEbA,GAAO,EACP,IAAI2D,EAAI,EACJC,EAAI,EACJC,EAAI1H,KAAK+G,SAAwB,EAAb/G,KAAK8G,MAC7B,MAAMa,EAAO,GACb,KAAOH,GAAKxH,KAAK8G,MAAOY,GAAK,EAAGD,GAAY,EA9OjC,IA8OwC,EAAJD,GAAQA,GAAK,EAAG,CAC7D,MAAMrH,EAAIsH,EAAIjB,EAAOP,EAAKyB,GACpBlJ,EAAIiJ,EAAIjB,EAAO3C,EAAK6D,GAC1B,GAAIlJ,EAAI2B,EAAIwH,EAAK5I,OAASiB,KAAK+D,aAC7B,MAAM,IAAI9F,MAAM,SAADiC,OACJ+F,EAAG,KAAA/F,OAAI2D,EAAG,oDAAA3D,OAAmDF,KAAK+G,SAAQ,YAAA7G,OAAWF,KAAK8G,MAAK,6DAG5Ga,EAAK9H,KAAK,CAACM,EAAG3B,GAChB,CACA,OAAOmJ,CACT,ECzPF,MAAMC,EACmB,qBAAhBC,YAA8B,IAAIA,YAAY,aAAU/G,EAcjE,SAASgH,EAAQC,GACf,OAAO,IAAIC,SAAQC,GAAWC,WAAWD,EAASF,IACpD,CACc,MAAOI,EAqBnBpI,WAAAA,CAAAsB,GAsBC,IAtBW,KACV+G,EAAI,WACJ9G,EAAU,QACV+G,EAAO,cACPC,EAAa,QACbC,EAAO,cACPC,EAAa,UACbC,EAAY,IAAG,eACfC,EAAiB,IAAQ,cACzBnH,EAAgBC,IAAKA,GAAC,eACtBmH,EAAiB,SAYlBtH,EACC,GAAIC,EACFtB,KAAKsB,WAAaA,MACb,KAAI8G,EAGT,MAAM,IAAIQ,UAAU,0CAFpB5I,KAAKsB,WAAa,IAAIuH,EAAAA,UAAUT,EAGlC,CAEA,GAAIE,EACFtI,KAAK8I,MAAQ,IAAIC,EAAI,CACnBzH,WAAYgH,EACZ/G,uBAEG,GAAIiH,EACTxI,KAAK8I,MAAQ,IAAIlC,EAAI,CACnBtF,WAAYkH,EACZjH,uBAEG,GAAI8G,EACTrI,KAAK8I,MAAQ,IAAIC,EAAI,CACnBzH,WAAY,IAAIuH,EAAAA,UAAUR,GAC1B9G,uBAEG,GAAIgH,EACTvI,KAAK8I,MAAQ,IAAIlC,EAAI,CACnBtF,WAAY,IAAIuH,EAAAA,UAAUN,GAC1BhH,sBAEG,KAAI6G,EAMT,MAAM,IAAIQ,UACR,yEANF5I,KAAK8I,MAAQ,IAAIC,EAAI,CACnBzH,WAAY,IAAIuH,EAAAA,UAAU,GAAD3I,OAAIkI,EAAI,SACjC7G,iBAMJ,CAEAvB,KAAK0I,eAAiBA,EACtB1I,KAAKyB,aAAeF,EACpBvB,KAAKyI,UAAYA,EACjBzI,KAAKgJ,WAAa,IAAIC,IAAJ,CAA4C,CAC5DC,MAAO,IAAIC,IAAJ,CAAQ,CAAEC,QAAS/C,KAAKM,MAAMgC,EAAiB,SACtDjE,KAAMA,CAAClE,EAAanC,IAClB2B,KAAKqJ,UAAU7I,EAAM,CAAEnC,YAE7B,CASA,cAAMiL,CACJ1G,EACA8E,EACAlJ,EACAmD,G,MAEA,IAAItD,EAEAkL,EADAC,EAAmB,CAAC,EAExB,QAAa1I,IAATa,EACF,MAAM,IAAIiH,UAAU,kCAStB,GAPoB,oBAATjH,EACT4H,EAAW5H,GAEX6H,EAAU7H,EACV4H,EAAW5H,EAAK8H,aAChBpL,EAASsD,EAAKtD,aAEAyC,IAAZ8B,EACF,MAAM,IAAIgG,UAAU,0CAEtB,IAAKW,EACH,MAAM,IAAIX,UAAU,kCAGtB,MAAMc,QAAiB1J,KAAK8I,MAAMpH,YAAY8H,GAC9CpL,EAAiBC,GACjB,MAAMuF,EAAS,OAAD8D,QAAC,IAADA,EAAAA,EAAK,EACb7D,EAAO,OAADrF,QAAC,IAADA,EAAAA,EAAKkL,EAASrE,aAC1B,KAAMzB,GAASC,GACb,MAAM,IAAI+E,UACR,8EAGJ,GAAIhF,IAAUC,EACZ,OAGF,MAAMlF,QAAeqB,KAAK8I,MAAMnD,eAAe/C,EAASgB,EAAOC,EAAK2F,GACpEpL,EAAiBC,GAIjB,IAAK,MAAMmB,KAASb,EAAQ,CAC1B,MAAMgL,EAAOnK,EAAMyB,cACnB,GAAI0I,EAAO3J,KAAK0I,eACd,MAAM,IAAIzK,MAAM,6BAADiC,OACgByJ,EAAKC,iBAAgB,qCAAA1J,OAAoCF,KAAK0I,eAAekB,iBAAgB,KAGhI,CAGA,IAAIC,EAAOC,KAAKC,MAChB,IAAK,MAAM7D,KAAKvH,EAAQ,CACtB,IAAIqL,EACJ,MAAM,OAAEC,EAAM,WAAEC,EAAU,WAAEC,SAAqBnK,KAAKgJ,WAAWoB,IAC/DlE,EAAEjG,WACFiG,EACA7H,GAGFD,EAAiBC,GACjB,IAAIgM,EAAa,EACbC,EAAM,EACV,KAAOD,EAAaJ,EAAOlL,QAAQ,CACjC,MAAMyC,EAAIyI,EAAOM,QAAQ,KAAMF,GAC/B,IAAW,IAAP7I,EACF,MAEF,MAAMrB,EAAI8J,EAAO1F,MAAM8F,EAAY7I,GAC7BgJ,EAAyB,QAAlBhI,EAAO,OAAPoF,QAAO,IAAPA,OAAO,EAAPA,EAAS6C,OAAOtK,UAAE,IAAAqC,EAAAA,EAAIrC,EAAEF,WAErC,GAAIkK,EAAY,CACd,KAAOE,EAAanE,EAAE9G,KAAKE,cAAgB6K,EAAWG,OACtDA,GACF,CAGA,MAAM,gBAAEI,EAAe,SAAEC,GAAa3K,KAAK4K,UACzClB,EACA9G,EACAgB,EACAC,EACA2G,GAKF,QAC8B1J,IAA5BkJ,QACoBlJ,IAApB4J,GACAV,EAA0BU,EAE1B,MAAM,IAAIzM,MAAM,yCAADiC,OAC4B8J,EAAuB,OAAA9J,OAAMwK,EAAe,2CAKzF,GAFAV,EAA0BU,EAEtBC,EACFpB,EACEiB,EAAKK,OASa,IAAlBX,EAAWI,IACRD,EAAaF,EAAWG,IACzBpE,EAAE9G,KAAKE,aACP,QAEC,QAAwBwB,IAApB4J,GAAiCA,GAAmB7G,EAI7D,OAIE7D,KAAKyI,WAAaoB,EAAOC,KAAKC,MAAQ/J,KAAKyI,YAC7CoB,EAAOC,KAAKC,MACZ3L,EAAiBC,SACXyJ,EAAQ,IAEhBuC,EAAa7I,EAAI,CACnB,CACF,CACF,CAEA,iBAAME,GAA8B,IAAlBC,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EACjC,OAAOP,KAAK8I,MAAMpH,YAAYC,EAChC,CAOA,qBAAMmJ,GAAkC,IAAlBnJ,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EACrC,MAAM,cAAEiE,EAAa,SAAER,EAAQ,aAAEsB,SACzBtF,KAAK0B,YAAYC,GACzBvD,EAAiBuD,EAAKtD,QACtB,MAAM0M,IAAyB,OAAbvG,QAAa,IAAbA,OAAa,EAAbA,EAAenF,gBAAiB,GAAKiG,EAIjDrC,QAAYjD,KAAKgL,YAAY,EAAGD,EAAUpJ,GAC1Cf,QAAcuC,EAAAA,EAAAA,OAAMF,GAG1B,GAAIe,EAAU,CAEZ,IAAIiH,GAAe,EACnB,MAAMC,EAAc,KAAKC,WAAW,GAC9BC,EAAWpH,EAASmH,WAAW,GACrC,IAAK,IAAI9K,EAAI,EAAGA,EAAIO,EAAM7B,SACpBsB,IAAM4K,EAAc,GAAKrK,EAAMP,KAAO+K,GADV/K,GAAK,EAIjCO,EAAMP,KAAO6K,IACfD,EAAc5K,GAGlB,OAAOO,EAAM2D,MAAM,EAAG0G,EAAc,EACtC,CACA,OAAOrK,CACT,CAQA,eAAMyK,GAA4B,IAAlB1J,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAE/B,aADoBP,KAAK8K,gBAAgBnJ,IAC5B1B,SAAS,OACxB,CAOA,+BAAMqL,GAA4C,IAAlB3J,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAE/C,aADuBP,KAAK0B,YAAYC,IACxB0C,WAClB,CAYAuG,SAAAA,CACElB,EACA6B,EACAC,EACAC,EACAjB,GAEA,MAAM,cAAE9G,EAAa,SAAEM,EAAQ,eAAER,EAAc,OAAEC,GAAWiG,EAE5D,GAAI1F,GAAYwG,EAAKkB,WAAW1H,GAC9B,MAAO,CAAE2G,UAAU,GAIrB,IAAI,IAAEhH,EAAG,MAAEC,EAAK,IAAEC,GAAQH,EACrBC,IACHA,EAAM,GAEHC,IACHA,EAAQ,GAELC,IACHA,EAAM,GAEO,QAAXJ,IACFI,EAAM,GAER,MAAM8H,EAAYtF,KAAKT,IAAIjC,EAAKC,EAAOC,GAMvC,IAAI+H,EAAsB,EACtBC,EAAqB,EACrBC,EAAS,GACTpB,GAAmBqB,IACvB,IAAK,IAAI1L,EAAI,EAAGA,EAAImK,EAAKzL,OAAS,EAAGsB,GAAK,EACxC,GAAgB,OAAZmK,EAAKnK,IAAeA,IAAMmK,EAAKzL,OAAQ,CACzC,GAAI6M,IAAwBjI,GAC1B,GACE3D,KAAKyB,aAAa+I,EAAKjG,MAAMsH,EAAoBxL,MACjDkL,EAEA,MAAO,CAAEZ,UAAU,QAEhB,GAAIiB,IAAwBhI,EAAO,CAMxC,GALA8G,EAAkBsB,SAASxB,EAAKjG,MAAMsH,EAAoBxL,GAAI,IAEvC,mBAAnBmD,IACFkH,GAAmB,GAEjBA,GAAmBe,EACrB,MAAO,CAAEf,kBAAiBC,UAAU,GAEtC,IAAY,IAAR9G,GAAaA,IAAQD,IAEnB8G,EAAkB,GAAKc,EACzB,MAAO,CAAEd,kBAAiBC,UAAU,EAG1C,MAAO,GAAe,QAAXlH,GAA4C,IAAxBmI,EAC7BE,EAAStB,EAAKjG,MAAMsH,EAAoBxL,QACnC,GAAIuL,IAAwB/H,EAAK,CAUtC,IAPa,QAAXJ,EACIzD,KAAKiM,WACHvB,EACAoB,EACAtB,EAAKjG,MAAMsH,EAAoBxL,IAEjC2L,SAASxB,EAAKjG,MAAMsH,EAAoBxL,GAAI,MAC7BmL,EACnB,MAAO,CAAEb,UAAU,EAEvB,CAGA,GAFAkB,EAAqBxL,EAAI,EACzBuL,GAAuB,EACnBA,EAAsBD,EACxB,KAEJ,CAEF,MAAO,CAAEjB,kBAAiBC,UAAU,EACtC,CAEAsB,UAAAA,CAAWvB,EAAyBoB,EAAgBI,GAClD,IAAIC,EAAgBzB,EAAkBoB,EAAO/M,OAM7C,MAAMqN,EAAQF,EAAKG,SAAS,cAC5B,GAAgB,MAAZH,EAAK,IAAeE,GAajB,GAAIA,EACT,OAAO1B,EAAkB,MAdI,CAC7B,IAAI4B,EAAW,IACf,IAAK,IAAIzH,EAAI,EAAGA,EAAIqH,EAAKnN,OAAQ8F,GAAK,EAAG,CACvC,GAAiB,MAAbyH,GAA6C,SAAzBJ,EAAK3H,MAAMM,EAAGA,EAAI,GAAe,CACvD,IAAI0H,EAAWL,EAAK3B,QAAQ,IAAK1F,IACf,IAAd0H,IACFA,EAAWL,EAAKnN,QAElBoN,EAAgBH,SAASE,EAAK3H,MAAMM,EAAI,EAAG0H,GAAW,IACtD,KACF,CACAD,EAAWJ,EAAKrH,EAClB,CACF,CAGA,OAAOsH,CACT,CAOA,eAAMxJ,CAAUC,GAAmC,IAAlBjB,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAChD,OAAOP,KAAK8I,MAAMnG,UAAUC,EAASjB,EACvC,CAEA,iBAAMqJ,CAAYV,EAAaX,GAAgC,IAAlBhI,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAC5D,MAAMJ,EAAIqM,EAAAA,OAAOC,MAAM9C,IACjB,UAAE+C,EAAS,OAAEzC,SAAiBjK,KAAKsB,WAAWqL,KAClDxM,EACA,EACAwJ,EACAW,EACA3I,GAGF,OAAOsI,EAAO1F,MAAM,EAAGmI,EACzB,CAMA,eAAMrD,CAAUnD,GAA4B,IAAlBvE,EAAApB,UAAAxB,OAAA,QAAA+B,IAAAP,UAAA,GAAAA,UAAA,GAAgB,CAAC,EAIzC,MAAMqM,QAAa5M,KAAKgL,YACtB9E,EAAE9G,KAAKC,cACP6G,EAAEjF,cACFU,GAEF,OAAOkL,EAAAA,EAAAA,iBAAgBD,EAAM1G,EAC/B,E","sources":["../node_modules/@gmod/tabix/src/util.ts","../node_modules/@gmod/tabix/src/virtualOffset.ts","../node_modules/@gmod/tabix/src/chunk.ts","../node_modules/@gmod/tabix/src/indexFile.ts","../node_modules/@gmod/tabix/src/tbi.ts","../node_modules/@gmod/tabix/src/csi.ts","../node_modules/@gmod/tabix/src/tabixIndexedFile.ts"],"sourcesContent":["import Chunk from './chunk'\nimport VirtualOffset from './virtualOffset'\n\nexport function longToNumber(long: Long) {\n  if (\n    long.greaterThan(Number.MAX_SAFE_INTEGER) ||\n    long.lessThan(Number.MIN_SAFE_INTEGER)\n  ) {\n    throw new Error('integer overflow')\n  }\n  return long.toNumber()\n}\n\nclass AbortError extends Error {\n  public code: string | undefined\n}\n/**\n * Properly check if the given AbortSignal is aborted.\n * Per the standard, if the signal reads as aborted,\n * this function throws either a DOMException AbortError, or a regular error\n * with a `code` attribute set to `ERR_ABORTED`.\n *\n * For convenience, passing `undefined` is a no-op\n *\n * @param {AbortSignal} [signal] an AbortSignal, or anything with an `aborted` attribute\n * @returns nothing\n */\nexport function checkAbortSignal(signal?: AbortSignal) {\n  if (!signal) {\n    return\n  }\n\n  if (signal.aborted) {\n    if (typeof DOMException !== 'undefined') {\n      // eslint-disable-next-line  no-undef\n      throw new DOMException('aborted', 'AbortError')\n    } else {\n      const e = new AbortError('aborted')\n      e.code = 'ERR_ABORTED'\n      throw e\n    }\n  }\n}\n\n/**\n * Skips to the next tick, then runs `checkAbortSignal`.\n * Await this to inside an otherwise synchronous loop to\n * provide a place to break when an abort signal is received.\n * @param {AbortSignal} signal\n */\nexport async function abortBreakPoint(signal?: AbortSignal) {\n  await Promise.resolve()\n  checkAbortSignal(signal)\n}\n\nexport function canMergeBlocks(chunk1: Chunk, chunk2: Chunk) {\n  return (\n    chunk2.minv.blockPosition - chunk1.maxv.blockPosition < 65000 &&\n    chunk2.maxv.blockPosition - chunk1.minv.blockPosition < 5000000\n  )\n}\n\nexport function optimizeChunks(chunks: Chunk[], lowest: VirtualOffset) {\n  const mergedChunks: Chunk[] = []\n  let lastChunk: Chunk | null = null\n\n  if (chunks.length === 0) {\n    return chunks\n  }\n\n  chunks.sort(function (c0, c1) {\n    const dif = c0.minv.blockPosition - c1.minv.blockPosition\n    return dif !== 0 ? dif : c0.minv.dataPosition - c1.minv.dataPosition\n  })\n\n  chunks.forEach(chunk => {\n    if (!lowest || chunk.maxv.compareTo(lowest) > 0) {\n      if (lastChunk === null) {\n        mergedChunks.push(chunk)\n        lastChunk = chunk\n      } else {\n        if (canMergeBlocks(lastChunk, chunk)) {\n          if (chunk.maxv.compareTo(lastChunk.maxv) > 0) {\n            lastChunk.maxv = chunk.maxv\n          }\n        } else {\n          mergedChunks.push(chunk)\n          lastChunk = chunk\n        }\n      }\n    }\n  })\n\n  return mergedChunks\n}\n","import { Buffer } from 'buffer'\nexport default class VirtualOffset {\n  public blockPosition: number\n  public dataPosition: number\n  constructor(blockPosition: number, dataPosition: number) {\n    this.blockPosition = blockPosition // < offset of the compressed data block\n    this.dataPosition = dataPosition // < offset into the uncompressed data\n  }\n\n  toString() {\n    return `${this.blockPosition}:${this.dataPosition}`\n  }\n\n  compareTo(b: VirtualOffset) {\n    return (\n      this.blockPosition - b.blockPosition || this.dataPosition - b.dataPosition\n    )\n  }\n\n  static min(...args: VirtualOffset[]) {\n    let min\n    let i = 0\n    for (; !min; i += 1) {\n      min = args[i]\n    }\n    for (; i < args.length; i += 1) {\n      if (min.compareTo(args[i]) > 0) {\n        min = args[i]\n      }\n    }\n    return min\n  }\n}\nexport function fromBytes(bytes: Buffer, offset = 0, bigendian = false) {\n  if (bigendian) {\n    throw new Error('big-endian virtual file offsets not implemented')\n  }\n\n  return new VirtualOffset(\n    bytes[offset + 7] * 0x10000000000 +\n      bytes[offset + 6] * 0x100000000 +\n      bytes[offset + 5] * 0x1000000 +\n      bytes[offset + 4] * 0x10000 +\n      bytes[offset + 3] * 0x100 +\n      bytes[offset + 2],\n    (bytes[offset + 1] << 8) | bytes[offset],\n  )\n}\n","import VirtualOffset from './virtualOffset'\n\n// little class representing a chunk in the index\nexport default class Chunk {\n  public minv: VirtualOffset\n  public maxv: VirtualOffset\n  public bin: number\n  public _fetchedSize?: number\n\n  constructor(\n    minv: VirtualOffset,\n    maxv: VirtualOffset,\n    bin: number,\n    fetchedSize = undefined,\n  ) {\n    this.minv = minv\n    this.maxv = maxv\n    this.bin = bin\n    this._fetchedSize = fetchedSize\n  }\n\n  toUniqueString() {\n    // eslint-disable-next-line @typescript-eslint/restrict-template-expressions\n    return `${this.minv}..${this.maxv} (bin ${\n      this.bin\n    }, fetchedSize ${this.fetchedSize()})`\n  }\n\n  toString() {\n    return this.toUniqueString()\n  }\n\n  compareTo(b: Chunk) {\n    return (\n      this.minv.compareTo(b.minv) ||\n      this.maxv.compareTo(b.maxv) ||\n      this.bin - b.bin\n    )\n  }\n\n  fetchedSize() {\n    if (this._fetchedSize !== undefined) {\n      return this._fetchedSize\n    }\n    return this.maxv.blockPosition + (1 << 16) - this.minv.blockPosition\n  }\n}\n","import { GenericFilehandle } from 'generic-filehandle'\nimport VirtualOffset from './virtualOffset'\nimport Chunk from './chunk'\n\nexport interface Options {\n  // support having some unknown parts of the options\n  [key: string]: unknown\n  signal?: AbortSignal\n}\n\nexport interface IndexData {\n  refNameToId: Record<string, number>\n  refIdToName: string[]\n  metaChar: string | null\n  columnNumbers: { ref: number; start: number; end: number }\n  coordinateType: string\n  format: string\n  [key: string]: any\n}\n\nexport default abstract class IndexFile {\n  public filehandle: GenericFilehandle\n  public renameRefSeq: (arg0: string) => string\n  private parseP?: Promise<IndexData>\n\n  constructor({\n    filehandle,\n    renameRefSeqs = (n: string) => n,\n  }: {\n    filehandle: GenericFilehandle\n    renameRefSeqs?: (a: string) => string\n  }) {\n    this.filehandle = filehandle\n    this.renameRefSeq = renameRefSeqs\n  }\n\n  public abstract lineCount(refName: string, args: Options): Promise<number>\n\n  protected abstract _parse(opts: Options): Promise<IndexData>\n\n  public async getMetadata(opts: Options = {}) {\n    const { indices: _indices, ...rest } = await this.parse(opts)\n    return rest\n  }\n\n  public abstract blocksForRange(\n    refName: string,\n    start: number,\n    end: number,\n    opts: Options,\n  ): Promise<Chunk[]>\n\n  _findFirstData(\n    currentFdl: VirtualOffset | undefined,\n    virtualOffset: VirtualOffset,\n  ) {\n    if (currentFdl) {\n      return currentFdl.compareTo(virtualOffset) > 0\n        ? virtualOffset\n        : currentFdl\n    } else {\n      return virtualOffset\n    }\n  }\n\n  async parse(opts: Options = {}) {\n    if (!this.parseP) {\n      this.parseP = this._parse(opts).catch(e => {\n        this.parseP = undefined\n        throw e\n      })\n    }\n    return this.parseP\n  }\n\n  async hasRefSeq(seqId: number, opts: Options = {}) {\n    const idx = await this.parse(opts)\n    return !!idx.indices[seqId]?.binIndex\n  }\n}\n","import Long from 'long'\nimport { Buffer } from 'buffer'\nimport VirtualOffset, { fromBytes } from './virtualOffset'\nimport Chunk from './chunk'\nimport { unzip } from '@gmod/bgzf-filehandle'\nimport { longToNumber, optimizeChunks, checkAbortSignal } from './util'\nimport IndexFile, { Options } from './indexFile'\n\nconst TBI_MAGIC = 21578324 // TBI\\1\nconst TAD_LIDX_SHIFT = 14\n\n/**\n * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n */\nfunction reg2bins(beg: number, end: number) {\n  beg += 1 // < convert to 1-based closed\n  end -= 1\n  return [\n    [0, 0],\n    [1 + (beg >> 26), 1 + (end >> 26)],\n    [9 + (beg >> 23), 9 + (end >> 23)],\n    [73 + (beg >> 20), 73 + (end >> 20)],\n    [585 + (beg >> 17), 585 + (end >> 17)],\n    [4681 + (beg >> 14), 4681 + (end >> 14)],\n  ]\n}\n\nexport default class TabixIndex extends IndexFile {\n  async lineCount(refName: string, opts: Options = {}) {\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return -1\n    }\n    const refId = indexData.refNameToId[refName]\n    const idx = indexData.indices[refId]\n    if (!idx) {\n      return -1\n    }\n    const { stats } = indexData.indices[refId]\n    if (stats) {\n      return stats.lineCount\n    }\n    return -1\n  }\n\n  // fetch and parse the index\n  async _parse(opts: Options = {}) {\n    const buf = await this.filehandle.readFile(opts)\n    const bytes = await unzip(buf)\n    checkAbortSignal(opts.signal)\n\n    // check TBI magic numbers\n    if (bytes.readUInt32LE(0) !== TBI_MAGIC /* \"TBI\\1\" */) {\n      throw new Error('Not a TBI file')\n      // TODO: do we need to support big-endian TBI files?\n    }\n\n    // number of reference sequences in the index\n    const refCount = bytes.readInt32LE(4)\n    const formatFlags = bytes.readInt32LE(8)\n    const coordinateType =\n      formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed'\n    const formatOpts: Record<number, string> = {\n      0: 'generic',\n      1: 'SAM',\n      2: 'VCF',\n    }\n    const format = formatOpts[formatFlags & 0xf]\n    if (!format) {\n      throw new Error(`invalid Tabix preset format flags ${formatFlags}`)\n    }\n    const columnNumbers = {\n      ref: bytes.readInt32LE(12),\n      start: bytes.readInt32LE(16),\n      end: bytes.readInt32LE(20),\n    }\n    const metaValue = bytes.readInt32LE(24)\n    const depth = 5\n    const maxBinNumber = ((1 << ((depth + 1) * 3)) - 1) / 7\n    const maxRefLength = 2 ** (14 + depth * 3)\n    const metaChar = metaValue ? String.fromCharCode(metaValue) : null\n    const skipLines = bytes.readInt32LE(28)\n\n    // read sequence dictionary\n    const nameSectionLength = bytes.readInt32LE(32)\n    const { refNameToId, refIdToName } = this._parseNameBytes(\n      bytes.slice(36, 36 + nameSectionLength),\n    )\n\n    // read the indexes for each reference sequence\n    let currOffset = 36 + nameSectionLength\n    let firstDataLine: VirtualOffset | undefined\n    const indices = new Array(refCount).fill(0).map(() => {\n      // the binning index\n      const binCount = bytes.readInt32LE(currOffset)\n      currOffset += 4\n      const binIndex: Record<number, Chunk[]> = {}\n      let stats\n      for (let j = 0; j < binCount; j += 1) {\n        const bin = bytes.readUInt32LE(currOffset)\n        currOffset += 4\n        if (bin > maxBinNumber + 1) {\n          throw new Error(\n            'tabix index contains too many bins, please use a CSI index',\n          )\n        } else if (bin === maxBinNumber + 1) {\n          const chunkCount = bytes.readInt32LE(currOffset)\n          currOffset += 4\n          if (chunkCount === 2) {\n            stats = this.parsePseudoBin(bytes, currOffset)\n          }\n          currOffset += 16 * chunkCount\n        } else {\n          const chunkCount = bytes.readInt32LE(currOffset)\n          currOffset += 4\n          const chunks = new Array(chunkCount)\n          for (let k = 0; k < chunkCount; k += 1) {\n            const u = fromBytes(bytes, currOffset)\n            const v = fromBytes(bytes, currOffset + 8)\n            currOffset += 16\n            firstDataLine = this._findFirstData(firstDataLine, u)\n            chunks[k] = new Chunk(u, v, bin)\n          }\n          binIndex[bin] = chunks\n        }\n      }\n\n      // the linear index\n      const linearCount = bytes.readInt32LE(currOffset)\n      currOffset += 4\n      const linearIndex = new Array(linearCount)\n      for (let k = 0; k < linearCount; k += 1) {\n        linearIndex[k] = fromBytes(bytes, currOffset)\n        currOffset += 8\n        firstDataLine = this._findFirstData(firstDataLine, linearIndex[k])\n      }\n      return { binIndex, linearIndex, stats }\n    })\n\n    return {\n      indices,\n      metaChar,\n      maxBinNumber,\n      maxRefLength,\n      skipLines,\n      firstDataLine,\n      columnNumbers,\n      coordinateType,\n      format,\n      refIdToName,\n      refNameToId,\n      maxBlockSize: 1 << 16,\n    }\n  }\n\n  parsePseudoBin(bytes: Buffer, offset: number) {\n    const lineCount = longToNumber(\n      Long.fromBytesLE(\n        bytes.slice(offset + 16, offset + 24) as unknown as number[],\n        true,\n      ),\n    )\n    return { lineCount }\n  }\n\n  _parseNameBytes(namesBytes: Buffer) {\n    let currRefId = 0\n    let currNameStart = 0\n    const refIdToName: string[] = []\n    const refNameToId: Record<string, number> = {}\n    for (let i = 0; i < namesBytes.length; i += 1) {\n      if (!namesBytes[i]) {\n        if (currNameStart < i) {\n          let refName = namesBytes.toString('utf8', currNameStart, i)\n          refName = this.renameRefSeq(refName)\n          refIdToName[currRefId] = refName\n          refNameToId[refName] = currRefId\n        }\n        currNameStart = i + 1\n        currRefId += 1\n      }\n    }\n    return { refNameToId, refIdToName }\n  }\n\n  async blocksForRange(\n    refName: string,\n    min: number,\n    max: number,\n    opts: Options = {},\n  ) {\n    if (min < 0) {\n      min = 0\n    }\n\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return []\n    }\n    const refId = indexData.refNameToId[refName]\n    const ba = indexData.indices[refId]\n    if (!ba) {\n      return []\n    }\n\n    const minOffset = ba.linearIndex.length\n      ? ba.linearIndex[\n          min >> TAD_LIDX_SHIFT >= ba.linearIndex.length\n            ? ba.linearIndex.length - 1\n            : min >> TAD_LIDX_SHIFT\n        ]\n      : new VirtualOffset(0, 0)\n    if (!minOffset) {\n      console.warn('querying outside of possible tabix range')\n    }\n\n    // const { linearIndex, binIndex } = indexes\n\n    const overlappingBins = reg2bins(min, max) // List of bin #s that overlap min, max\n    const chunks: Chunk[] = []\n\n    // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n    for (const [start, end] of overlappingBins) {\n      for (let bin = start; bin <= end; bin++) {\n        if (ba.binIndex[bin]) {\n          for (const c of ba.binIndex[bin]) {\n            chunks.push(new Chunk(c.minv, c.maxv, bin))\n          }\n        }\n      }\n    }\n\n    // Use the linear index to find minimum file position of chunks that could\n    // contain alignments in the region\n    const nintv = ba.linearIndex.length\n    let lowest = null\n    const minLin = Math.min(min >> 14, nintv - 1)\n    const maxLin = Math.min(max >> 14, nintv - 1)\n    for (let i = minLin; i <= maxLin; ++i) {\n      const vp = ba.linearIndex[i]\n      if (vp) {\n        if (!lowest || vp.compareTo(lowest) < 0) {\n          lowest = vp\n        }\n      }\n    }\n\n    return optimizeChunks(chunks, lowest)\n  }\n}\n","import Long from 'long'\nimport { Buffer } from 'buffer'\nimport { unzip } from '@gmod/bgzf-filehandle'\n\nimport VirtualOffset, { fromBytes } from './virtualOffset'\nimport Chunk from './chunk'\nimport { longToNumber, optimizeChunks } from './util'\n\nimport IndexFile, { Options } from './indexFile'\n\nconst CSI1_MAGIC = 21582659 // CSI\\1\nconst CSI2_MAGIC = 38359875 // CSI\\2\n\nfunction lshift(num: number, bits: number) {\n  return num * 2 ** bits\n}\nfunction rshift(num: number, bits: number) {\n  return Math.floor(num / 2 ** bits)\n}\n\nexport default class CSI extends IndexFile {\n  private maxBinNumber: number\n  private depth: number\n  private minShift: number\n  constructor(args: any) {\n    super(args)\n    this.maxBinNumber = 0\n    this.depth = 0\n    this.minShift = 0\n  }\n  async lineCount(refName: string, opts: Options = {}): Promise<number> {\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return -1\n    }\n    const refId = indexData.refNameToId[refName]\n    const idx = indexData.indices[refId]\n    if (!idx) {\n      return -1\n    }\n    const { stats } = indexData.indices[refId]\n    if (stats) {\n      return stats.lineCount\n    }\n    return -1\n  }\n  indexCov() {\n    throw new Error('CSI indexes do not support indexcov')\n  }\n\n  parseAuxData(bytes: Buffer, offset: number) {\n    const formatFlags = bytes.readInt32LE(offset)\n    const coordinateType =\n      formatFlags & 0x10000 ? 'zero-based-half-open' : '1-based-closed'\n    const format = { 0: 'generic', 1: 'SAM', 2: 'VCF' }[formatFlags & 0xf]\n    if (!format) {\n      throw new Error(`invalid Tabix preset format flags ${formatFlags}`)\n    }\n    const columnNumbers = {\n      ref: bytes.readInt32LE(offset + 4),\n      start: bytes.readInt32LE(offset + 8),\n      end: bytes.readInt32LE(offset + 12),\n    }\n    const metaValue = bytes.readInt32LE(offset + 16)\n    const metaChar = metaValue ? String.fromCharCode(metaValue) : null\n    const skipLines = bytes.readInt32LE(offset + 20)\n    const nameSectionLength = bytes.readInt32LE(offset + 24)\n\n    const { refIdToName, refNameToId } = this._parseNameBytes(\n      bytes.slice(offset + 28, offset + 28 + nameSectionLength),\n    )\n\n    return {\n      refIdToName,\n      refNameToId,\n      skipLines,\n      metaChar,\n      columnNumbers,\n      format,\n      coordinateType,\n    }\n  }\n\n  _parseNameBytes(namesBytes: Buffer) {\n    let currRefId = 0\n    let currNameStart = 0\n    const refIdToName = []\n    const refNameToId: Record<string, number> = {}\n    for (let i = 0; i < namesBytes.length; i += 1) {\n      if (!namesBytes[i]) {\n        if (currNameStart < i) {\n          let refName = namesBytes.toString('utf8', currNameStart, i)\n          refName = this.renameRefSeq(refName)\n          refIdToName[currRefId] = refName\n          refNameToId[refName] = currRefId\n        }\n        currNameStart = i + 1\n        currRefId += 1\n      }\n    }\n    return { refNameToId, refIdToName }\n  }\n\n  // fetch and parse the index\n\n  async _parse(opts: Options = {}) {\n    const bytes = await unzip(await this.filehandle.readFile(opts))\n\n    // check TBI magic numbers\n    let csiVersion\n    if (bytes.readUInt32LE(0) === CSI1_MAGIC) {\n      csiVersion = 1\n    } else if (bytes.readUInt32LE(0) === CSI2_MAGIC) {\n      csiVersion = 2\n    } else {\n      throw new Error('Not a CSI file')\n      // TODO: do we need to support big-endian CSI files?\n    }\n\n    this.minShift = bytes.readInt32LE(4)\n    this.depth = bytes.readInt32LE(8)\n    this.maxBinNumber = ((1 << ((this.depth + 1) * 3)) - 1) / 7\n    const maxRefLength = 2 ** (this.minShift + this.depth * 3)\n    const auxLength = bytes.readInt32LE(12)\n    const aux =\n      auxLength && auxLength >= 30\n        ? this.parseAuxData(bytes, 16)\n        : {\n            refIdToName: [],\n            refNameToId: {},\n            metaChar: null,\n            columnNumbers: { ref: 0, start: 1, end: 2 },\n            coordinateType: 'zero-based-half-open',\n            format: 'generic',\n          }\n    const refCount = bytes.readInt32LE(16 + auxLength)\n\n    // read the indexes for each reference sequence\n    let firstDataLine: VirtualOffset | undefined\n    let currOffset = 16 + auxLength + 4\n    const indices = new Array(refCount).fill(0).map(() => {\n      // the binning index\n      const binCount = bytes.readInt32LE(currOffset)\n      currOffset += 4\n      const binIndex: Record<string, Chunk[]> = {}\n      let stats // < provided by parsing a pseudo-bin, if present\n      for (let j = 0; j < binCount; j += 1) {\n        const bin = bytes.readUInt32LE(currOffset)\n        if (bin > this.maxBinNumber) {\n          // this is a fake bin that actually has stats information\n          // about the reference sequence in it\n          stats = this.parsePseudoBin(bytes, currOffset + 4)\n          currOffset += 4 + 8 + 4 + 16 + 16\n        } else {\n          const loffset = fromBytes(bytes, currOffset + 4)\n          firstDataLine = this._findFirstData(firstDataLine, loffset)\n          const chunkCount = bytes.readInt32LE(currOffset + 12)\n          currOffset += 16\n          const chunks = new Array(chunkCount)\n          for (let k = 0; k < chunkCount; k += 1) {\n            const u = fromBytes(bytes, currOffset)\n            const v = fromBytes(bytes, currOffset + 8)\n            currOffset += 16\n            // this._findFirstData(data, u)\n            chunks[k] = new Chunk(u, v, bin)\n          }\n          binIndex[bin] = chunks\n        }\n      }\n\n      return { binIndex, stats }\n    })\n\n    return {\n      ...aux,\n      csi: true,\n      refCount,\n      maxBlockSize: 1 << 16,\n      firstDataLine,\n      csiVersion,\n      indices,\n      depth: this.depth,\n      maxBinNumber: this.maxBinNumber,\n      maxRefLength,\n    }\n  }\n\n  parsePseudoBin(bytes: Buffer, offset: number) {\n    const lineCount = longToNumber(\n      Long.fromBytesLE(\n        bytes.slice(offset + 28, offset + 36) as unknown as number[],\n        true,\n      ),\n    )\n    return { lineCount }\n  }\n\n  async blocksForRange(\n    refName: string,\n    min: number,\n    max: number,\n    opts: Options = {},\n  ) {\n    if (min < 0) {\n      min = 0\n    }\n\n    const indexData = await this.parse(opts)\n    if (!indexData) {\n      return []\n    }\n    const refId = indexData.refNameToId[refName]\n    const ba = indexData.indices[refId]\n    if (!ba) {\n      return []\n    }\n\n    // const { linearIndex, binIndex } = indexes\n\n    const overlappingBins = this.reg2bins(min, max) // List of bin #s that overlap min, max\n    const chunks: Chunk[] = []\n\n    // Find chunks in overlapping bins.  Leaf bins (< 4681) are not pruned\n    for (const [start, end] of overlappingBins) {\n      for (let bin = start; bin <= end; bin++) {\n        if (ba.binIndex[bin]) {\n          for (const c of ba.binIndex[bin]) {\n            chunks.push(new Chunk(c.minv, c.maxv, bin))\n          }\n        }\n      }\n    }\n\n    return optimizeChunks(chunks, new VirtualOffset(0, 0))\n  }\n\n  /**\n   * calculate the list of bins that may overlap with region [beg,end) (zero-based half-open)\n   */\n  reg2bins(beg: number, end: number) {\n    beg -= 1 // < convert to 1-based closed\n    if (beg < 1) {\n      beg = 1\n    }\n    if (end > 2 ** 50) {\n      end = 2 ** 34\n    } // 17 GiB ought to be enough for anybody\n    end -= 1\n    let l = 0\n    let t = 0\n    let s = this.minShift + this.depth * 3\n    const bins = []\n    for (; l <= this.depth; s -= 3, t += lshift(1, l * 3), l += 1) {\n      const b = t + rshift(beg, s)\n      const e = t + rshift(end, s)\n      if (e - b + bins.length > this.maxBinNumber) {\n        throw new Error(\n          `query ${beg}-${end} is too large for current binning scheme (shift ${this.minShift}, depth ${this.depth}), try a smaller query or a coarser index binning scheme`,\n        )\n      }\n      bins.push([b, e])\n    }\n    return bins\n  }\n}\n","import AbortablePromiseCache from 'abortable-promise-cache'\nimport LRU from 'quick-lru'\nimport { Buffer } from 'buffer'\nimport { GenericFilehandle, LocalFile } from 'generic-filehandle'\nimport { unzip, unzipChunkSlice } from '@gmod/bgzf-filehandle'\nimport { checkAbortSignal } from './util'\nimport IndexFile, { Options, IndexData } from './indexFile'\n\nimport Chunk from './chunk'\nimport TBI from './tbi'\nimport CSI from './csi'\n\ntype GetLinesCallback = (line: string, fileOffset: number) => void\n\nconst decoder =\n  typeof TextDecoder !== 'undefined' ? new TextDecoder('utf8') : undefined\n\ninterface GetLinesOpts {\n  [key: string]: unknown\n  signal?: AbortSignal\n  lineCallback: GetLinesCallback\n}\n\ninterface ReadChunk {\n  buffer: Buffer\n  cpositions: number[]\n  dpositions: number[]\n}\n\nfunction timeout(time: number) {\n  return new Promise(resolve => setTimeout(resolve, time))\n}\nexport default class TabixIndexedFile {\n  private filehandle: GenericFilehandle\n  private index: IndexFile\n  private chunkSizeLimit: number\n  private yieldTime: number\n  private renameRefSeq: (n: string) => string\n  private chunkCache: AbortablePromiseCache<Chunk, ReadChunk>\n\n  /**\n   * @param {object} args\n   * @param {string} [args.path]\n   * @param {filehandle} [args.filehandle]\n   * @param {string} [args.tbiPath]\n   * @param {filehandle} [args.tbiFilehandle]\n   * @param {string} [args.csiPath]\n   * @param {filehandle} [args.csiFilehandle]\n   * @param {number} [args.yieldTime] yield to main thread after N milliseconds if reading features is taking a long time to avoid hanging main thread\n   * @param {function} [args.renameRefSeqs] optional function with sig `string => string` to transform\n   * reference sequence names for the purpose of indexing and querying. note that the data that is returned is\n   * not altered, just the names of the reference sequences that are used for querying.\n   */\n  constructor({\n    path,\n    filehandle,\n    tbiPath,\n    tbiFilehandle,\n    csiPath,\n    csiFilehandle,\n    yieldTime = 500,\n    chunkSizeLimit = 50000000,\n    renameRefSeqs = n => n,\n    chunkCacheSize = 5 * 2 ** 20,\n  }: {\n    path?: string\n    filehandle?: GenericFilehandle\n    tbiPath?: string\n    tbiFilehandle?: GenericFilehandle\n    csiPath?: string\n    csiFilehandle?: GenericFilehandle\n    yieldTime?: number\n    chunkSizeLimit?: number\n    renameRefSeqs?: (n: string) => string\n    chunkCacheSize?: number\n  }) {\n    if (filehandle) {\n      this.filehandle = filehandle\n    } else if (path) {\n      this.filehandle = new LocalFile(path)\n    } else {\n      throw new TypeError('must provide either filehandle or path')\n    }\n\n    if (tbiFilehandle) {\n      this.index = new TBI({\n        filehandle: tbiFilehandle,\n        renameRefSeqs,\n      })\n    } else if (csiFilehandle) {\n      this.index = new CSI({\n        filehandle: csiFilehandle,\n        renameRefSeqs,\n      })\n    } else if (tbiPath) {\n      this.index = new TBI({\n        filehandle: new LocalFile(tbiPath),\n        renameRefSeqs,\n      })\n    } else if (csiPath) {\n      this.index = new CSI({\n        filehandle: new LocalFile(csiPath),\n        renameRefSeqs,\n      })\n    } else if (path) {\n      this.index = new TBI({\n        filehandle: new LocalFile(`${path}.tbi`),\n        renameRefSeqs,\n      })\n    } else {\n      throw new TypeError(\n        'must provide one of tbiFilehandle, tbiPath, csiFilehandle, or csiPath',\n      )\n    }\n\n    this.chunkSizeLimit = chunkSizeLimit\n    this.renameRefSeq = renameRefSeqs\n    this.yieldTime = yieldTime\n    this.chunkCache = new AbortablePromiseCache<Chunk, ReadChunk>({\n      cache: new LRU({ maxSize: Math.floor(chunkCacheSize / (1 << 16)) }),\n      fill: (args: Chunk, signal?: AbortSignal) =>\n        this.readChunk(args, { signal }),\n    })\n  }\n\n  /**\n   * @param refName name of the reference sequence\n   * @param start start of the region (in 0-based half-open coordinates)\n   * @param end end of the region (in 0-based half-open coordinates)\n   * @param opts callback called for each line in the region. can also pass a object param containing obj.lineCallback, obj.signal, etc\n   * @returns promise that is resolved when the whole read is finished, rejected on error\n   */\n  async getLines(\n    refName: string,\n    s: number | undefined,\n    e: number | undefined,\n    opts: GetLinesOpts | GetLinesCallback,\n  ) {\n    let signal: AbortSignal | undefined\n    let options: Options = {}\n    let callback: (line: string, lineOffset: number) => void\n    if (opts === undefined) {\n      throw new TypeError('line callback must be provided')\n    }\n    if (typeof opts === 'function') {\n      callback = opts\n    } else {\n      options = opts\n      callback = opts.lineCallback\n      signal = opts.signal\n    }\n    if (refName === undefined) {\n      throw new TypeError('must provide a reference sequence name')\n    }\n    if (!callback) {\n      throw new TypeError('line callback must be provided')\n    }\n\n    const metadata = await this.index.getMetadata(options)\n    checkAbortSignal(signal)\n    const start = s ?? 0\n    const end = e ?? metadata.maxRefLength\n    if (!(start <= end)) {\n      throw new TypeError(\n        'invalid start and end coordinates. start must be less than or equal to end',\n      )\n    }\n    if (start === end) {\n      return\n    }\n\n    const chunks = await this.index.blocksForRange(refName, start, end, options)\n    checkAbortSignal(signal)\n\n    // check the chunks for any that are over the size limit.  if\n    // any are, don't fetch any of them\n    for (const chunk of chunks) {\n      const size = chunk.fetchedSize()\n      if (size > this.chunkSizeLimit) {\n        throw new Error(\n          `Too much data. Chunk size ${size.toLocaleString()} bytes exceeds chunkSizeLimit of ${this.chunkSizeLimit.toLocaleString()}.`,\n        )\n      }\n    }\n\n    // now go through each chunk and parse and filter the lines out of it\n    let last = Date.now()\n    for (const c of chunks) {\n      let previousStartCoordinate: number | undefined\n      const { buffer, cpositions, dpositions } = await this.chunkCache.get(\n        c.toString(),\n        c,\n        signal,\n      )\n\n      checkAbortSignal(signal)\n      let blockStart = 0\n      let pos = 0\n      while (blockStart < buffer.length) {\n        const n = buffer.indexOf('\\n', blockStart)\n        if (n === -1) {\n          break\n        }\n        const b = buffer.slice(blockStart, n)\n        const line = decoder?.decode(b) ?? b.toString()\n\n        if (dpositions) {\n          while (blockStart + c.minv.dataPosition >= dpositions[pos++]) {}\n          pos--\n        }\n\n        // filter the line for whether it is within the requested range\n        const { startCoordinate, overlaps } = this.checkLine(\n          metadata,\n          refName,\n          start,\n          end,\n          line,\n        )\n\n        // do a small check just to make sure that the lines are really sorted\n        // by start coordinate\n        if (\n          previousStartCoordinate !== undefined &&\n          startCoordinate !== undefined &&\n          previousStartCoordinate > startCoordinate\n        ) {\n          throw new Error(\n            `Lines not sorted by start coordinate (${previousStartCoordinate} > ${startCoordinate}), this file is not usable with Tabix.`,\n          )\n        }\n        previousStartCoordinate = startCoordinate\n\n        if (overlaps) {\n          callback(\n            line.trim(),\n            // cpositions[pos] refers to actual file offset of a bgzip block boundaries\n            //\n            // we multiply by (1 <<8) in order to make sure each block has a \"unique\"\n            // address space so that data in that block could never overlap\n            //\n            // then the blockStart-dpositions is an uncompressed file offset from\n            // that bgzip block boundary, and since the cpositions are multiplied by\n            // (1 << 8) these uncompressed offsets get a unique space\n            cpositions[pos] * (1 << 8) +\n              (blockStart - dpositions[pos]) +\n              c.minv.dataPosition +\n              1,\n          )\n        } else if (startCoordinate !== undefined && startCoordinate >= end) {\n          // the lines were overlapping the region, but now have stopped, so\n          // we must be at the end of the relevant data and we can stop\n          // processing data now\n          return\n        }\n\n        // yield if we have emitted beyond the yield limit\n        if (this.yieldTime && last - Date.now() > this.yieldTime) {\n          last = Date.now()\n          checkAbortSignal(signal)\n          await timeout(1)\n        }\n        blockStart = n + 1\n      }\n    }\n  }\n\n  async getMetadata(opts: Options = {}) {\n    return this.index.getMetadata(opts)\n  }\n\n  /**\n   * get a buffer containing the \"header\" region of\n   * the file, which are the bytes up to the first\n   * non-meta line\n   */\n  async getHeaderBuffer(opts: Options = {}) {\n    const { firstDataLine, metaChar, maxBlockSize } =\n      await this.getMetadata(opts)\n    checkAbortSignal(opts.signal)\n    const maxFetch = (firstDataLine?.blockPosition || 0) + maxBlockSize\n    // TODO: what if we don't have a firstDataLine, and the header\n    // actually takes up more than one block? this case is not covered here\n\n    const buf = await this._readRegion(0, maxFetch, opts)\n    const bytes = await unzip(buf)\n\n    // trim off lines after the last non-meta line\n    if (metaChar) {\n      // trim backward from the end\n      let lastNewline = -1\n      const newlineByte = '\\n'.charCodeAt(0)\n      const metaByte = metaChar.charCodeAt(0)\n      for (let i = 0; i < bytes.length; i += 1) {\n        if (i === lastNewline + 1 && bytes[i] !== metaByte) {\n          break\n        }\n        if (bytes[i] === newlineByte) {\n          lastNewline = i\n        }\n      }\n      return bytes.slice(0, lastNewline + 1)\n    }\n    return bytes\n  }\n\n  /**\n   * get a string containing the \"header\" region of the\n   * file, is the portion up to the first non-meta line\n   *\n   * @returns {Promise} for a string\n   */\n  async getHeader(opts: Options = {}) {\n    const bytes = await this.getHeaderBuffer(opts)\n    return bytes.toString('utf8')\n  }\n\n  /**\n   * get an array of reference sequence names, in the order in which\n   * they occur in the file. reference sequence renaming is not applied\n   * to these names.\n   */\n  async getReferenceSequenceNames(opts: Options = {}) {\n    const metadata = await this.getMetadata(opts)\n    return metadata.refIdToName\n  }\n\n  /**\n   * @param {object} metadata metadata object from the parsed index,\n   * containing columnNumbers, metaChar, and format\n   * @param {string} regionRefName\n   * @param {number} regionStart region start coordinate (0-based-half-open)\n   * @param {number} regionEnd region end coordinate (0-based-half-open)\n   * @param {array[string]} line\n   * @returns {object} like `{startCoordinate, overlaps}`. overlaps is boolean,\n   * true if line is a data line that overlaps the given region\n   */\n  checkLine(\n    metadata: IndexData,\n    regionRefName: string,\n    regionStart: number,\n    regionEnd: number,\n    line: string,\n  ) {\n    const { columnNumbers, metaChar, coordinateType, format } = metadata\n    // skip meta lines\n    if (metaChar && line.startsWith(metaChar)) {\n      return { overlaps: false }\n    }\n\n    // check ref/start/end using column metadata from index\n    let { ref, start, end } = columnNumbers\n    if (!ref) {\n      ref = 0\n    }\n    if (!start) {\n      start = 0\n    }\n    if (!end) {\n      end = 0\n    }\n    if (format === 'VCF') {\n      end = 8\n    }\n    const maxColumn = Math.max(ref, start, end)\n\n    // this code is kind of complex, but it is fairly fast.\n    // basically, we want to avoid doing a split, because if the lines are really long\n    // that could lead to us allocating a bunch of extra memory, which is slow\n\n    let currentColumnNumber = 1 // cols are numbered starting at 1 in the index metadata\n    let currentColumnStart = 0\n    let refSeq = ''\n    let startCoordinate = -Infinity\n    for (let i = 0; i < line.length + 1; i += 1) {\n      if (line[i] === '\\t' || i === line.length) {\n        if (currentColumnNumber === ref) {\n          if (\n            this.renameRefSeq(line.slice(currentColumnStart, i)) !==\n            regionRefName\n          ) {\n            return { overlaps: false }\n          }\n        } else if (currentColumnNumber === start) {\n          startCoordinate = parseInt(line.slice(currentColumnStart, i), 10)\n          // we convert to 0-based-half-open\n          if (coordinateType === '1-based-closed') {\n            startCoordinate -= 1\n          }\n          if (startCoordinate >= regionEnd) {\n            return { startCoordinate, overlaps: false }\n          }\n          if (end === 0 || end === start) {\n            // if we have no end, we assume the feature is 1 bp long\n            if (startCoordinate + 1 <= regionStart) {\n              return { startCoordinate, overlaps: false }\n            }\n          }\n        } else if (format === 'VCF' && currentColumnNumber === 4) {\n          refSeq = line.slice(currentColumnStart, i)\n        } else if (currentColumnNumber === end) {\n          // this will never match if there is no end column\n          const endCoordinate =\n            format === 'VCF'\n              ? this._getVcfEnd(\n                  startCoordinate,\n                  refSeq,\n                  line.slice(currentColumnStart, i),\n                )\n              : parseInt(line.slice(currentColumnStart, i), 10)\n          if (endCoordinate <= regionStart) {\n            return { overlaps: false }\n          }\n        }\n        currentColumnStart = i + 1\n        currentColumnNumber += 1\n        if (currentColumnNumber > maxColumn) {\n          break\n        }\n      }\n    }\n    return { startCoordinate, overlaps: true }\n  }\n\n  _getVcfEnd(startCoordinate: number, refSeq: string, info: any) {\n    let endCoordinate = startCoordinate + refSeq.length\n    // ignore TRA features as they specify CHR2 and END as being on a different\n    // chromosome\n    //\n    // if CHR2 is on the same chromosome, still ignore it because there should\n    // be another pairwise feature at the end of this one\n    const isTRA = info.includes('SVTYPE=TRA')\n    if (info[0] !== '.' && !isTRA) {\n      let prevChar = ';'\n      for (let j = 0; j < info.length; j += 1) {\n        if (prevChar === ';' && info.slice(j, j + 4) === 'END=') {\n          let valueEnd = info.indexOf(';', j)\n          if (valueEnd === -1) {\n            valueEnd = info.length\n          }\n          endCoordinate = parseInt(info.slice(j + 4, valueEnd), 10)\n          break\n        }\n        prevChar = info[j]\n      }\n    } else if (isTRA) {\n      return startCoordinate + 1\n    }\n    return endCoordinate\n  }\n\n  /**\n   * return the approximate number of data lines in the given reference sequence\n   * @param refSeq reference sequence name\n   * @returns number of data lines present on that reference sequence\n   */\n  async lineCount(refName: string, opts: Options = {}) {\n    return this.index.lineCount(refName, opts)\n  }\n\n  async _readRegion(pos: number, size: number, opts: Options = {}) {\n    const b = Buffer.alloc(size)\n    const { bytesRead, buffer } = await this.filehandle.read(\n      b,\n      0,\n      size,\n      pos,\n      opts,\n    )\n\n    return buffer.slice(0, bytesRead)\n  }\n\n  /**\n   * read and uncompress the data in a chunk (composed of one or more\n   * contiguous bgzip blocks) of the file\n   */\n  async readChunk(c: Chunk, opts: Options = {}) {\n    // fetch the uncompressed data, uncompress carefully a block at a time,\n    // and stop when done\n\n    const data = await this._readRegion(\n      c.minv.blockPosition,\n      c.fetchedSize(),\n      opts,\n    )\n    return unzipChunkSlice(data, c)\n  }\n}\n"],"names":["longToNumber","long","greaterThan","Number","MAX_SAFE_INTEGER","lessThan","MIN_SAFE_INTEGER","Error","toNumber","AbortError","checkAbortSignal","signal","aborted","DOMException","e","code","optimizeChunks","chunks","lowest","mergedChunks","lastChunk","length","sort","c0","c1","dif","minv","blockPosition","dataPosition","forEach","chunk","chunk1","chunk2","maxv","compareTo","push","VirtualOffset","constructor","this","toString","concat","b","min","i","_len","arguments","args","Array","_key","fromBytes","bytes","offset","undefined","Chunk","bin","fetchedSize","_fetchedSize","toUniqueString","IndexFile","_ref","filehandle","renameRefSeqs","n","renameRefSeq","getMetadata","opts","indices","_indices","rest","parse","_findFirstData","currentFdl","virtualOffset","parseP","_parse","catch","hasRefSeq","seqId","_a","binIndex","TabixIndex","lineCount","refName","indexData","refId","refNameToId","stats","buf","readFile","unzip","readUInt32LE","refCount","readInt32LE","formatFlags","coordinateType","format","columnNumbers","ref","start","end","metaValue","maxBinNumber","metaChar","String","fromCharCode","skipLines","nameSectionLength","refIdToName","_parseNameBytes","slice","firstDataLine","currOffset","fill","map","binCount","j","chunkCount","parsePseudoBin","k","u","v","linearCount","linearIndex","maxRefLength","maxBlockSize","Long","namesBytes","currRefId","currNameStart","blocksForRange","max","ba","console","warn","overlappingBins","beg","c","nintv","minLin","Math","maxLin","vp","rshift","num","bits","floor","CSI","super","depth","minShift","indexCov","parseAuxData","csiVersion","auxLength","aux","loffset","csi","reg2bins","l","t","s","bins","decoder","TextDecoder","timeout","time","Promise","resolve","setTimeout","TabixIndexedFile","path","tbiPath","tbiFilehandle","csiPath","csiFilehandle","yieldTime","chunkSizeLimit","chunkCacheSize","TypeError","LocalFile","index","TBI","chunkCache","AbortablePromiseCache","cache","LRU","maxSize","readChunk","getLines","callback","options","lineCallback","metadata","size","toLocaleString","last","Date","now","previousStartCoordinate","buffer","cpositions","dpositions","get","blockStart","pos","indexOf","line","decode","startCoordinate","overlaps","checkLine","trim","getHeaderBuffer","maxFetch","_readRegion","lastNewline","newlineByte","charCodeAt","metaByte","getHeader","getReferenceSequenceNames","regionRefName","regionStart","regionEnd","startsWith","maxColumn","currentColumnNumber","currentColumnStart","refSeq","Infinity","parseInt","_getVcfEnd","info","endCoordinate","isTRA","includes","prevChar","valueEnd","Buffer","alloc","bytesRead","read","data","unzipChunkSlice"],"sourceRoot":""}